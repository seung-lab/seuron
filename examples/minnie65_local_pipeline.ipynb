{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d2bc52-085d-4b23-8caa-e31a5c8bace2",
   "metadata": {},
   "source": [
    "Inference, segmentation and synapse detection locally\n",
    "==================================\n",
    "This tutorial goes through steps to run inference and segmentation and synapse detection jobs using seuron deployed locally with docker compose. You should only use this notebook in jupyterlab hosted by seuron, and in order to run the examples, you need a linux system with a recent NVidia GPU with **CUDA 11.8+**. Make sure you follow the steps in the README file to create the local deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3cf1c-54ca-4b02-826c-f12580f919a5",
   "metadata": {},
   "source": [
    "Load seuronbot extension\n",
    "========================\n",
    "_The seuronbot extension can only be loaded by one notebook, trying to load another instance of the extension will result in an error. If you want to use the extension in a different notebook, make sure you reset the kernel of the current notebook to unload the extension_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f150c427-50fe-4f49-9954-d62e33505541",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext seuronbot_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d96385b-dfbe-4c3b-b240-693b67a4f76d",
   "metadata": {},
   "source": [
    "Visualize a small cutout in Minnie65\n",
    "=====================================\n",
    "In order to demonstrate the pipeline, we use a small cutout from Minnie65, which is the image stack FlyWire is based on. The bounding box of the cutout is [129285, 95090, 21304, 129797, 95602, 21496] at 8nm x 8nm x 40nm voxel size. The bbox is all we need to run the pipeline. In the cell below, The cutout are visualized using neuroglancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b2dd30-b4d3-4187-bded-62d7e616ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, HTML\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "ng_payload = {\n",
    "  \"dimensions\": {\n",
    "    \"x\": [8, \"nm\"],\n",
    "    \"y\": [8, \"nm\"],\n",
    "    \"z\": [40, \"nm\"],\n",
    "  },\n",
    "  \"position\": [129445, 95290, 21354],\n",
    "  \"layers\": [\n",
    "    {\n",
    "      \"type\": \"image\",\n",
    "      \"source\": \"precomputed://https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\",\n",
    "      \"tab\": \"rendering\",\n",
    "      \"name\": \"minnie65\"\n",
    "    },\n",
    "    {\n",
    "      \"type\": \"annotation\",\n",
    "      \"source\": {\"url\": \"local://annotations\"},\n",
    "      \"tool\": \"annotateBoundingBox\",\n",
    "      \"tab\": \"annotations\",\n",
    "      \"annotations\": [\n",
    "        {\n",
    "          \"pointA\": [129285, 95090, 21304],\n",
    "          \"pointB\": [129797, 95602, 21496],\n",
    "          \"type\": \"axis_aligned_bounding_box\",\n",
    "          \"id\": \"6c88d2d28baae4c93a4d26736c039a8fc3c7d1ca\"\n",
    "        }\n",
    "      ],\n",
    "      \"name\": \"annotation\"\n",
    "    }\n",
    "  ],\n",
    "  \"showAxisLines\": False,\n",
    "  \"showSlices\": False,\n",
    "  \"layout\": \"xy-3d\"\n",
    "}\n",
    "ng_link = f\"https://neuroglancer-demo.appspot.com/#!{urllib.parse.quote(json.dumps(ng_payload))}\"\n",
    "display(IFrame(src=ng_link, width=800, height=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4925c5a-fb39-4c57-a822-686193be0778",
   "metadata": {},
   "source": [
    "Run inference to create affinity map\n",
    "====================================\n",
    "To segment the cutout we picked, we first need to generate affinity map for it. The procedure is divided into two steps: First one submits the parameters describing the tasks, using the `update parameters` command. The bot performs sanity checks to prevent common mistakes. After the sanity check succeeds. One can launch the pipeline with `run pipeline` commands.\n",
    "\n",
    "Submit inference parameters\n",
    "--------------------------------------------------------\n",
    "The command will send the python code in the current cell to seuronbot. The cell content must be **self-contained**, no reference to variables or modules from other cells. The cell have to define a `submit_parameter` function requiring no arguments. The function should return a dictionary containing the inference parameters or a list of dictionaries for several tasks. The bot will execute this function in a separate environment and use the returned parameters for sanity checks.\n",
    "\n",
    "Seuron uses [the zettaai fork of chunkflow](https://github.com/ZettaAI/chunkflow) for inference tasks, `ranlu/chunkflow:zettaai` pointing to the latest build from the zettaiai. The parameters for each run can be divided into roughly two types: parameters for IO and parameters for the inference model. The IO parameters contain information of the input images: the location, resolution and bounding box, the output will be stored in `OUTPUT_PATH`, or `{OUTPUT_PREFIX}{NAME}` when `OUTPUT_PATH` is not defined. \n",
    "The parameters for the inference model must be adjusted for each model. You can find the parameters for the models we used below from [DeepEM](https://github.com/seung-lab/DeepEM/releases/tag/minnie) and [modelzoo](https://github.com/seung-lab/modelzoo/tree/main/models/IARPAphase2)\n",
    "\n",
    "For local processes, we store the final and intermediate results to `/tmp`, which is automatically mounted from the host for all containers managed by seuron. If you want to use some other volume, make sure you add them to the deployment file and have proper permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d8c9b-b05a-4bae-b906-4a00b396864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%seuronbot update parameters\n",
    "def submit_inference_parameters():\n",
    "    bbox = [129285, 95090, 21304, 129797, 95602, 21496]\n",
    "    common_param = {\n",
    "        \"IMAGE_PATH\": \"https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\",\n",
    "        \"IMAGE_FILL_MISSING\": True,\n",
    "        \"IMAGE_RESOLUTION\": [8, 8, 40],\n",
    "        \"BBOX\": bbox,\n",
    "        \"OUTPUT_PREFIX\": \"file:///tmp/scratch/ng/test_aff/\",\n",
    "        \"INHERIT_PARAMETERS\": False,\n",
    "        \"CHUNKFLOW_IMAGE\": \"ranlu/chunkflow:trt\",\n",
    "        \"MAX_RAM\": 1,\n",
    "    }\n",
    "    aff_param = {\n",
    "        \"INPUT_PATCH_SIZE\": [256,256,20],\n",
    "        \"OUTPUT_PATCH_SIZE\": [192,192,16],\n",
    "        \"INFERENCE_OUTPUT_CHANNELS\": 4,\n",
    "        \"OUTPUT_CHANNELS\": 3,\n",
    "        \"MYELIN_MASK_THRESHOLD\": 0.3,\n",
    "        \"INPUT_PATCH_OVERLAP_RATIO\": 0.5,\n",
    "        \"ONNX_MODEL_PATH\": \"https://github.com/seung-lab/DeepEM/releases/download/minnie/mye-crop.onnx\",\n",
    "    }\n",
    "    sem_param = {\n",
    "        \"INPUT_PATCH_SIZE\": [256,256,20],\n",
    "        \"INFERENCE_OUTPUT_CHANNELS\": 5,\n",
    "        \"OUTPUT_CHANNELS\": 1,\n",
    "        \"OUTPUT_DTYPE\": \"uint8\",\n",
    "        \"OUTPUT_LAYER_TYPE\": \"segmentation\",\n",
    "        \"POSTPROC\": \"channel-voting\",\n",
    "        \"INPUT_PATCH_OVERLAP_RATIO\": 0.5,\n",
    "        \"ONNX_MODEL_PATH\": \"https://github.com/seung-lab/modelzoo/raw/main/models/IARPAphase2/SemanticSegmentation/model.onnx\"\n",
    "    }\n",
    "    syn_param = {\n",
    "        \"INPUT_PATCH_SIZE\": [192,192,18],\n",
    "        \"INFERENCE_OUTPUT_CHANNELS\": 1,\n",
    "        \"INPUT_PATCH_OVERLAP_RATIO\": 0.2,\n",
    "        \"ONNX_MODEL_PATH\": \"https://github.com/seung-lab/modelzoo/raw/main/models/IARPAphase2/SynapseDetection/model.onnx\"\n",
    "    }\n",
    "    return [\n",
    "                {\n",
    "                    \"NAME\": \"aff_local_test\",\n",
    "                    **common_param,\n",
    "                    **aff_param,\n",
    "                },\n",
    "                {\n",
    "                    \"NAME\": \"sem_local_test\",\n",
    "                    **common_param,\n",
    "                    **sem_param,\n",
    "                },\n",
    "                {\n",
    "                    \"NAME\": \"syn_local_test\",\n",
    "                    **common_param,\n",
    "                    **syn_param,\n",
    "                },\n",
    "           ]\n",
    "\n",
    "def submit_parameters():\n",
    "    inf_param = submit_inference_parameters()\n",
    "    return inf_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee74a33-fc84-469b-a365-9d8a1794aad5",
   "metadata": {},
   "source": [
    "Launch inference run\n",
    "--------------------\n",
    "Once the sanity check succeeded, we can start the inference using `%seuronbot run pipeline` command. The first message after the run is triggered is the cancel token to use if you want to cancel the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85ee31-baf8-4b96-9bac-36637b7e01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%seuronbot run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905eec1-53d1-40d6-9da0-0f78a3d65637",
   "metadata": {},
   "source": [
    "Segment the affinity map\n",
    "========================\n",
    "After the affinity map is generated, we use it to create segmentation of the Flywire cutout. The procedure is similar to the inference tasks, first use `update parameter` command to submit the parameters describing the tasks, then use `run pipeline` to trigger the run.\n",
    "\n",
    "Submit segmentation parameters\n",
    "----------------------------------\n",
    "For segmentation tasks seuron uses [abiss](https://github.com/seung-lab/abiss), `ranlu/abiss:main` is built from the main branch. The command creates both flat segmentations and inputs for [PyChunkedGraph](https://github.com/seung-lab/PyChunkedGraph). The segmentation requires affinity map as an input, make sure you update the path to the affinity map if you modified it in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a124c6-91c5-43e8-9a3b-0227ab88ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%seuronbot update parameters\n",
    "\n",
    "def submit_segmentation_parameters(aff_path, sem_path):\n",
    "    io_param = {\n",
    "        \"SCRATCH_PREFIX\": \"file:///tmp/scratch/\",\n",
    "        \"IMAGE_PATH\": \"https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\",\n",
    "        \"AFF_PATH\": aff_path,\n",
    "        \"SEM_PATH\": sem_path,\n",
    "        \"NG_PREFIX\": \"file:///tmp/scratch/ng/\",\n",
    "    }\n",
    "    seg_param = {\n",
    "        \"WS_HIGH_THRESHOLD\": \"0.99\",\n",
    "        \"WS_LOW_THRESHOLD\": \"0.01\",\n",
    "        \"WS_SIZE_THRESHOLD\": \"200\",\n",
    "        \"AGG_THRESHOLD\": \"0.27\",\n",
    "        \"WORKER_IMAGE\": \"ranlu/abiss:main\",\n",
    "        \"SKIP_SKELETON\": True,\n",
    "    }\n",
    "    return [\n",
    "                {\n",
    "                    \"NAME\": f\"seg_local_test\",\n",
    "                    **io_param,\n",
    "                    **seg_param,\n",
    "                },\n",
    "           ]\n",
    "\n",
    "def submit_parameters():\n",
    "    aff_path = \"file:///tmp/scratch/ng/test_aff/aff_local_test\"\n",
    "    sem_path = \"file:///tmp/scratch/ng/test_aff/sem_local_test\"\n",
    "    run_param = submit_segmentation_parameters(aff_path, sem_path)\n",
    "    return run_param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d92a09-bd17-42cb-b70c-fbdafb72ccf7",
   "metadata": {},
   "source": [
    "Launch segmentation run\n",
    "-----------------------\n",
    "Same as inference, we can start the segmentation using `%seuronbot run pipeline` command. And you will receive the cancel token at the starting of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2cefc6-6081-4326-82a3-86407c76b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%seuronbot run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bd4a3-11f0-4ebc-ad64-6cfb6b658e3e",
   "metadata": {},
   "source": [
    "Synapse Detection\n",
    "=================\n",
    "With the segmentation and voxel level synapses prediction, we use [Synaptor](https://github.com/ZettaAI/Synaptor) to detect synapses in the test cutout. The workflow is similar to inference and segmentation: submit the parameters using the `update parameters` then start the run with `run pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7084797-4762-4052-bd7e-8022f4594e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%seuronbot update parameters\n",
    "def submit_synaptor_parameters():\n",
    "    synaptor_param = {\n",
    "        \"Dimensions\": {\n",
    "          \"blockshape\": \"64, 64, 64\",\n",
    "          \"chunkshape\": \"256, 256, 64\",\n",
    "          \"patchshape\": \"80, 80, 18\",\n",
    "          \"startcoord\": \"129285, 95090, 21304\",\n",
    "          \"volshape\": \"512, 512, 192\",\n",
    "          \"voxelres\": \"8, 8, 40\"\n",
    "        },\n",
    "        \"Parameters\": {\n",
    "          \"ccthresh\": 0.27,\n",
    "          \"dustthresh\": 0,\n",
    "          \"mergethresh\": 0,\n",
    "          \"nummergetasks\": 1,\n",
    "          \"szthresh\": 40\n",
    "        },\n",
    "        \"Provenance\": {\n",
    "          \"motivation\": \"Testing postsynaptic terminal predictions\"\n",
    "        },\n",
    "        \"Volumes\": {\n",
    "          \"baseseg\": \"file:///tmp/scratch/ng/seg/seg_local_test\",\n",
    "          \"descriptor\": \"file:///tmp/scratch/ng/test_aff/syn_local_test\",\n",
    "          \"image\": \"https://bossdb-open-data.s3.amazonaws.com/iarpa_microns/minnie/minnie65/em\",\n",
    "          \"output\": \"file:///tmp/scratch/ng/seg/syn_local_test\",\n",
    "          \"tempoutput\": \"file:///tmp/scratch/ng/seg/syn_tmp\"\n",
    "        },\n",
    "        \"Workflow\": {\n",
    "          \"connectionstr\": \"None\",\n",
    "          \"maxclustersize\": 1,\n",
    "          \"modelpath\": \"https://github.com/seung-lab/modelzoo/raw/main/models/IARPAphase2/SynapseAssignment/model.onnx\",\n",
    "          \"queuename\": \"SHOULD_BE_SET_BY_AIRFLOW\",\n",
    "          \"queueurl\": \"SHOULD_BE_SET_BY_AIRFLOW\",\n",
    "          \"storagedir\": \"/tmp/scratch/ng/seg/syn_output/\",\n",
    "          \"synaptor_image\": \"ranlu/synaptor:onnx\",\n",
    "          \"workflowtype\": \"Segmentation+Assignment\",\n",
    "          \"workspacetype\": \"File\"\n",
    "        }\n",
    "    }\n",
    "    return synaptor_param\n",
    "\n",
    "def submit_parameters():\n",
    "    inf_param = submit_synaptor_parameters()\n",
    "    return inf_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde0020-3d87-490c-9e71-c5c39a848f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%seuronbot run pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
