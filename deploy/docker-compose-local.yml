version: '3.7'
x-airflow-common:
    &airflow-common
    image: ${SEURON_TAG}
    restart: always
    environment:
        &airflow-common-env
        VENDOR:
        AIRFLOW__CORE__FERNET_KEY:
        AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
        AIRFLOW__CELERY__BROKER_URL:
        AIRFLOW__CELERY__CELERY_RESULT_BACKEND:
        AIRFLOW__WEBSERVER__SECRET_KEY:
        AIRFLOW__LOGGING__BASE_LOG_FOLDER:
        AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER:
        AIRFLOW__METRICS__STATSD_ON:
        AIRFLOW__METRICS__STATSD_HOST:
        AIRFLOW__METRICS__STATSD_PORT:
        REDIS_SERVER:
    volumes:
        # Add local input data here
        # - /my/local/data:/path/to/mount
        - /var/run/docker.sock:/var/run/docker.sock
        - /tmp:/tmp

services:
    worker-manager:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
            AIRFLOW__CELERY__WORKER_CONCURRENCY: 3
        command: airflow celery worker --without-gossip --without-mingle -q manager
        deploy:
            restart_policy:
                condition: any

    worker-gpu:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
            AIRFLOW__CELERY__WORKER_CONCURRENCY: 1
        command: airflow celery worker --without-gossip --without-mingle -q gpu
        deploy:
            restart_policy:
                condition: any

    worker-atomic:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
            AIRFLOW__CELERY__WORKER_CONCURRENCY: 1
        command: airflow celery worker --without-gossip --without-mingle -q atomic
        deploy:
            restart_policy:
                condition: any

    worker-igneous:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
        command: python custom/task_execution.py --queue igneous --concurrency 1
        deploy:
            restart_policy:
                condition: any

    worker-custom-cpu:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
        command: custom/worker_cpu.sh 4
        deploy:
            restart_policy:
                condition: any

    cv-viewer:
        <<: *airflow-common
        environment:
            <<: *airflow-common-env
        command: python utils/cv_viewer.py file:///tmp/scratch/ng
        ports:
          - "8080:8080"
        deploy:
            restart_policy:
                condition: any

# Enable this services to import secrets needed to access cloud services, for
# segmentation GCS requires google-secret.json and gsutil-secret.json. AWS
# requires aws-secret.json and s3-secret
#
#     add-secrets:
#         <<: *airflow-common
#         restart: on-failure
#         command: python scripts/secrets_to_airflow_variables.py
#         depends_on:
#             init-seuronbot:
#                 condition: service_completed_successfully
#         secrets:
#             - google-secret.json
#             - gsutil-secret.json
#             - aws-secret.json
#             - s3-secret
#
# secrets:
#     google-secret.json:
#         file: ~/.cloudvolume/secrets/google-secret.json
#     gsutil-secret.json:
#         file: ~/.cloudvolume/secrets/google-secret.json
#     aws-secret.json:
#         file: ~/.cloudvolume/secrets/aws-secret.json
#     s3-secret:
#         file: ~/.aws/credentials
